{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HtOoDiJZOXp"
      },
      "source": [
        "# Advanced Spark\n",
        "**Andrey Titov**  \n",
        "andrey.titov@bigdatateam.org  \n",
        "Big Data Instructor @ BigData Team  \n",
        "https://bigdatateam.org"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHSyFQ6-ZOXy"
      },
      "source": [
        "## На этом занятии\n",
        "+ Партиционирование\n",
        "+ Планы выполнения задач\n",
        "+ Оптимизация соединений и группировок\n",
        "+ Управление схемой данных\n",
        "+ Оптимизатор запросов Catalyst"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87DAcpb0ZOX0"
      },
      "source": [
        "## Партиционирование\n",
        "RDD и DF являются представляют собой классы, описывающие распределенные коллекции данных. Они (коллекции) разбиты на крупные блоки, которые называются партициями. В графе вычисления, который называется в Spark DAG (Direct Acyclic Graph), есть три основных компонента - `job`, `stage`, `task`.\n",
        "\n",
        "`job` представляет собой весь граф целиком, от момента создания DF, до применения `action` к нему. Состоит из одной или более `stage`. Когда возникает необходимость сделать `shuffle` данных, Spark создает новый `stage`. Каждый `stage` состоит из большого количества `task`. `task` это базовая операция над данными. Одновременно Spark выполняет N `task`, которые обрабатывают N партиций, где N - это суммарное число доступных потоков на всех воркерах.\n",
        "\n",
        "Исходя из этого, важно обеспечивать:\n",
        "+ достаточное количество партиций для распределения нагрузки по всем воркерам\n",
        "+ равномерное распределение данных между партициями\n",
        "\n",
        "Создадим датасет с перекосом данных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0ULY1TPZOX2"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when, lit, col\n",
        "\n",
        "skew_column = when(col(\"id\") < 900, lit(0)).otherwise(lit(1)).alias(\"skew_column\")\n",
        "\n",
        "skewed_df = spark.range(1000).withColumn(\"skew\", skew_column).repartition(10, col(\"skew\"))\n",
        "\n",
        "skewed_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFSjYtgQZOX6"
      },
      "outputs": [],
      "source": [
        "def print_parts(df):\n",
        "    ret = df.rdd.mapPartitions(lambda x: [len(list(x))]).collect()\n",
        "    print(ret)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6WTUR0GZOX7"
      },
      "outputs": [],
      "source": [
        "print_parts(skewed_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZnqyeGQZOX9"
      },
      "source": [
        "Любые операции с таким датасетом будут работать медленно, т.к.\n",
        "+ если суммарное количество потоков на всех воркерах больше 10, то в один момент времени работать будут максимум 10, остальные будут простаивать\n",
        "+ из 10 партицийи только в 2 есть данные и это означает, что только 2 потока будут обрабатывать данные, при этом из-за перекоса данных между ними (900 vs 100) первый станет bottleneck'ом\n",
        "\n",
        "Обычно перекошенные датасеты возникают после вычисления агрегатов, оконных функций и соединений, но также могут возникать и при чтении источников.\n",
        "\n",
        "Для устранения проблемы перекоса данных, следует использовать метод `repartition`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTWlmq0QZOX_"
      },
      "outputs": [],
      "source": [
        "# здесь мы передаем только новое количество партиций и Spark выполнит RoundRobinPartitioning\n",
        "\n",
        "balanced_df = skewed_df.repartition(20)\n",
        "print_parts(balanced_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qMXZlKCZOYB"
      },
      "outputs": [],
      "source": [
        "# здесь мы добавляем к числу партиций колонку, по которой необходимо сделать репартиционирование,\n",
        "# поэтому Spark выполнит HashPartitioning\n",
        "\n",
        "balanced_df = skewed_df.repartition(20, col(\"id\"))\n",
        "print_parts(balanced_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3ybCjnDZOYD"
      },
      "source": [
        "### Добавление соли\n",
        "Часто при вычислении агрегатов приходится работать с перекошенными данными:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOfZ53KnZOYF"
      },
      "outputs": [],
      "source": [
        "df = spark.read.format(\"csv\").options(header=True, inferSchema=True).load(\"/tmp/datasets/airport-codes.csv\")\n",
        "df.groupBy(col(\"type\")).count().orderBy(col(\"count\").desc()).show(30, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q7yjqbbZOYH"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import collect_list, col\n",
        "\n",
        "skew_grouped = df.groupBy(col(\"type\")).agg(collect_list(col(\"ident\")).alias(\"ids\"))\n",
        "skew_grouped.show(20, 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeFBmFigZOYI"
      },
      "source": [
        "Поскольку при вычислении агрегата происходит неявный `HashPartitioning` по ключу (ключам) агрегата, то при выполнении определенных условий происходит нехватка памяти на воркере, которую нельзя исправить, не изменив подход к построению агрегата.\n",
        "\n",
        "Один из вариантов устранение - соление ключей:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hvQ8C0ZZOYJ"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "salt = expr(\"\"\"pmod(round(rand() * 100, 0), 10)\"\"\").cast(\"integer\")\n",
        "salted = df.withColumn(\"salt\", salt)\n",
        "salted.select(col(\"type\"), col(\"ident\"), col(\"salt\")).sample(0.1).show(20, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EHRln__ZOYK"
      },
      "source": [
        "Это позволяет нам существенно снизить объем данных в каждой партиции (30к vs 3к):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ZQKLRxiyZOYK"
      },
      "outputs": [],
      "source": [
        "salted.groupBy(col(\"type\"), col(\"salt\")).count().orderBy(col(\"count\").desc()).show(20, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNCW3yB_ZOYL"
      },
      "source": [
        "Это позволяет нам посчитать требуемый агрегат более оптимальным путем, не смотря на появление второго агрегата:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWkCMkKAZOYM"
      },
      "outputs": [],
      "source": [
        "salted \\\n",
        "    .groupBy(col(\"type\"), col(\"salt\")).agg(collect_list(col(\"ident\")).alias(\"ids\")) \\\n",
        "    .groupBy(col(\"type\")).agg(collect_list(col(\"ids\")).alias(\"ids\")) \\\n",
        "    .select(col(\"type\"), expr(\"\"\"flatten(ids)\"\"\").alias(\"ids\")) \\\n",
        "    .show(20, 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkkiBSAEZOYN"
      },
      "source": [
        "### Выводы:\n",
        "+ DF API позволяет строить большое количество агрегатов. При этом необходимо помнить, что операции `groupBy`, `cube`, `rollup` возвращают [org.apache.spark.sql.RelationalGroupedDataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset), к которому затем необходимо применить одну из функций агрегации - `count`, `sum`, `agg` и т. п.\n",
        "+ При вычислении агрегатов необходимо помнить, что эта операция требует перемешивания данных между воркерами, что, в случае перекошенных данных, может привести к OOM на воркере.\n",
        "\n",
        "## Кеширование\n",
        "По умолчанию при применении каждого действия Spark пересчитывает весь граф, что может негативно сказать на производительности приложения. Для демонстрации возьмем датасет [Airport Codes](https://datahub.io/core/airport-codes)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PdDTyHUZOYO"
      },
      "outputs": [],
      "source": [
        "df\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AboEX3a2ZOYP"
      },
      "source": [
        "Посчитаем несколько действий. Несмотря на то, что `only_ru` является общим для всех действий, он пересчитывается при вызове каждого действия."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3unMzK2wZOYP"
      },
      "outputs": [],
      "source": [
        "only_ru = df.filter((col(\"iso_country\") == \"RU\") & (col(\"elevation_ft\") > 1000))\n",
        "only_ru.show(1, 50, True)\n",
        "\n",
        "only_ru.count()\n",
        "only_ru.collect()\n",
        "only_ru.groupBy(col(\"municipality\")).count().orderBy(col(\"count\").desc()).na.drop(\"any\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty_XOeUsZOYQ"
      },
      "source": [
        "Для решения этой проблемы следует использовать методы `cache`, либо `persist`. Данные методы сохраняют состояние графа после первого действия, и следующие обращаются к нему. Разница между методами заключается в том, что `persist` позволяет выбрать, куда сохранить данные, а `cache` использует значение по умолчанию. В текущей версии Spark это [StorageLevel.MEMORY_ONLY](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence). Важно помнить, что данный кеш не предназначен для обмена данными между разными Spark приложения - он является внутренним для приложения. После того, как работа с данными окончена, необходимо выполнить `unpersist` для очистки памяти"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ns9J8omWZOYR"
      },
      "outputs": [],
      "source": [
        "only_ru = df.filter((col(\"iso_country\") == \"RU\") & (col(\"elevation_ft\") > 1000))\n",
        "only_ru.cache()\n",
        "only_ru.show(1, 50, True)\n",
        "only_ru.count()\n",
        "only_ru.collect()\n",
        "only_ru.groupBy(col(\"municipality\")).count().orderBy(col(\"count\").desc()).na.drop(\"any\").show()\n",
        "only_ru.unpersist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPOoyX2qZOYR"
      },
      "source": [
        "### Выводы:\n",
        "+ Использование `cache` и `persist` позволяет существенно сократить время обработки данных, однако следует помнить и об увеличении потребляемой памяти на воркерах"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHso6ilUZOYS"
      },
      "source": [
        "## Планы выполнения задач\n",
        "\n",
        "Любой `job` в Spark SQL имеет под собой план выполнения, кототорый генерируется на основе написанно запроса. План запроса содержит операторы, которые затем превращаются в Java код. Поскольку одну и ту же задачу в Spark SQL можно выполнить по-разному, полезно смотреть в планы выполнения, чтобы, например:\n",
        "+ убрать лишние shuffle\n",
        "+ убедиться, чтот тот или иной оператор будет выполнен на уровне источника, а не внутри Spark\n",
        "+ понять, как будет выполнен `join`\n",
        "\n",
        "Планы выполнения доступны в двух видах:\n",
        "+ метод `explain()` у DF\n",
        "+ на вкладке SQL в Spark UI\n",
        "\n",
        "Прочитаем датасет [Airport Codes](https://datahub.io/core/airport-codes):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enDhv6j3ZOYT"
      },
      "outputs": [],
      "source": [
        "df = spark.read.format(\"csv\").options(header=True, inferSchema=True).load(\"/tmp/datasets/airport-codes.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDaFLlRBZOYT"
      },
      "source": [
        "Используем метод `explain`, чтобы посмотреть план запроса. Наиболее интересным является физический план, т.к. он отражает фактически алгоритм обработки данных. В данном случае в плане присутствует единственный оператор `FileScan csv`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2pE3XcuZOYU"
      },
      "outputs": [],
      "source": [
        "df.explain(extended=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuESOdyKZOYV"
      },
      "source": [
        "Выполним `filter` и проверим план выполнения. Читать план нужно снизу вверх. В плане появился новый оператор `filter`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2bsPaj_ZOYV"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df.filter(col(\"type\") == \"small_airport\").explain(extended=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc6Fk1UjZOYW"
      },
      "source": [
        "Выполним агрегацию и проверим план выполнения. В нем появляется три оператора: 2 `HashAggregate` и `Exchange hashpartitioning`.\n",
        "\n",
        "Первый `HashAggregate` содержит функцию `partial_count(1)`. Это означает, что внутри каждого воркера произойдет подсчет строк по каждому ключу. Затем происходит `shuffle` по ключу агрегата, после которого выполняется еще один `HashAggregate` с функцией `count(1)`. Использование двух `HashAggregate` позволяет сократить количество передаваемых данных по сети."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8geFB_7KZOYW"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df.filter(col(\"type\") == \"small_airport\").groupBy(col(\"iso_country\")).count().explain(extended=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3LGQg1dZOYX"
      },
      "source": [
        "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://cs5.pikabu.ru/post_img/big/2015/12/11/7/1449830295198229367.jpg\">\n",
        "\n",
        "### Выводы:\n",
        "+ Spark составляет физический план выполнения запроса на основании написанного вами кода\n",
        "+ Изучив план запроса, можно понять, какие операторы будут применены в ходе обработки ваших данных\n",
        "+ План выполнения запроса - один из основных инструментов оптимизации запроса\n",
        "\n",
        "## Оптимизация соединений и группировок\n",
        "При выполнении `join` двух DF важно следовать рекомендациям:\n",
        "+ фильтровать данные до join'а\n",
        "+ использовать equ join \n",
        "+ если можно путем увеличения количества данных применить equ join вместо non-equ join'а, то делать именно так\n",
        "+ всеми силами избегать cross-join'ов\n",
        "+ если правый DF помещается в памяти worker'а, использовать broadcast()\n",
        "\n",
        "### Виды соединений\n",
        "+ **BroadcastHashJoin**\n",
        "  - equ join\n",
        "  - broadcast\n",
        "+ **SortMergeJoin**\n",
        "  - equ join\n",
        "  - sortable keys\n",
        "+ **BroadcastNestedLoopJoin**\n",
        "  - non-equ join\n",
        "  - using broadcast\n",
        "+ **CartesianProduct**\n",
        "  - non-equ join\n",
        "  \n",
        "[Optimizing Apache Spark SQL Joins: Spark Summit East talk by Vida Ha](https://youtu.be/fp53QhSfQcI)\n",
        "\n",
        "Подготовим два датасета:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPQaLd4iZOYX"
      },
      "outputs": [],
      "source": [
        "left = df.select(col(\"type\"), col(\"ident\"), col(\"iso_country\")).alias(\"left\").localCheckpoint()\n",
        "right = df.groupBy(col(\"type\")).count().alias(\"right\").localCheckpoint()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUKH33cfZOYY"
      },
      "source": [
        "### BroadcastHashJoin\n",
        "+ работает, когда условие - равенство одного или нескольких ключей\n",
        "+ работает, когда один из датасетов небольшой и полностью вмещается в память воркера\n",
        "+ оставляет левый датасет как есть\n",
        "+ копирует правый датасет на каждый воркер\n",
        "+ составляет hash map из правого датасета, где ключ - кортеж из колонок в условии соединения\n",
        "+ итерируется по левому датасета внутри каждой партиции и проверяет наличие ключей в HashMap\n",
        "+ может быть автоматически использован, либо явно через `broadcast(df)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b9liSvkZOYZ"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "result = left.join(broadcast(right), \"type\", \"inner\")\n",
        "\n",
        "result.explain(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZQQkeuRZOYa"
      },
      "source": [
        "### SortMergeJoin\n",
        "+ работает, когда ключи соединения в обоих датасета являются сортируемыми\n",
        "+ репартиционирует оба датасета в 200 партиций по ключу (ключам) соединения\n",
        "+ сортирует партиции каждого из датасетов по ключу (ключам) соединения\n",
        "+ Используя сравнение левого и правого ключей, обходит каждую пару партиций и соединяет строки с одинаковыми ключами"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCN-iH_vZOYa"
      },
      "outputs": [],
      "source": [
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
        "\n",
        "result = left.join(right, \"type\", \"inner\")\n",
        "\n",
        "result.explain(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7WaTmJDZOYb"
      },
      "source": [
        "### BroadcastNestedLoopJoin\n",
        "+ работает, когда один из датасетов небольшой и полностью вмещается в память воркера\n",
        "+ оставляет левый датасет как есть\n",
        "+ копирует правый датасет на каждый воркер\n",
        "+ проходится вложенным циклом по каждой партиции левого датасета и копией правого датасета и проверяет условие\n",
        "+ может быть автоматически использован, либо явно через `broadcast(df)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhIB4T3NZOYb"
      },
      "outputs": [],
      "source": [
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
        "\n",
        "result = left.join(broadcast(right), left[\"type\"] != right[\"type\"], \"inner\")\n",
        "\n",
        "result.explain(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWzpyol4ZOYc"
      },
      "source": [
        "### CartesianProduct\n",
        "+ Создает пары из каждой партиции левого датасета с каждой партицией правого датасета, релоцирует каждую пару на один воркер и проверяет условие соединения\n",
        "+ на выходе создает N*M партиций\n",
        "+ работает медленнее остальных и часто приводит к ООМ воркеров"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fG_3jggCZOYc"
      },
      "outputs": [],
      "source": [
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
        "\n",
        "result = left.join(right, left[\"type\"] != right[\"type\"], \"inner\")\n",
        "\n",
        "result.explain(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG-VmwBWZOYd"
      },
      "source": [
        "### Снижение количества shuffle\n",
        "В ряде случаев можно уйти от лишних `shuffle` операций при выполнении соединения. Для этого оба DF должны иметь одинаковое партиционирование - одинаковое количество партиций и ключ партиционирования, совпадающий с ключом соединения.\n",
        "\n",
        "Разница между планами выполнения будет хорошо видна в Spark UI на графе выполнения в Jobs и плане выполнения в SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WII3xRp-ZOYd"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "left = df\n",
        "right = df.groupBy(col(\"type\")).count()\n",
        "joined = left.join(right, \"type\")\n",
        "joined.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1AHvczXZOYe"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "df_rep = df.repartition(200, col(\"type\"))\n",
        "left = df_rep\n",
        "right = df_rep.groupBy(col(\"type\")).count()\n",
        "joined = left.join(right, \"type\")\n",
        "joined.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmcOMV8UZOYf"
      },
      "source": [
        "### Выводы:\n",
        "+ В Spark используются 4 вида соединений: `BroadcastHashJoin`, `SortMergeJoin`, `BroadcastNestedLoopJoin`, `CartesianProduct`\n",
        "+ Выбор алгоритма основывается на условии соединения и размере датасетов\n",
        "+ `CartesianProduct` обладает самой низкой вычислительной эффективностью и его по возможности стоит избегать\n",
        "\n",
        "## Управление схемой данных\n",
        "В DF API каждая колонка имеет свой тип. Он может быть:\n",
        "+ скаляром - `StringType`, `IntegerType` и т. д.\n",
        "+ массивом - `ArrayType(T)`\n",
        "+ словарем `MapType(K, V)`\n",
        "+ структурой - `StructType()`\n",
        "\n",
        "DF целиком также имеет схему, описанную с помощью класса `StructType`\n",
        "\n",
        "Посмотреть список колонок можно с помощью атрибута `columns`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCDAwinpZOYf"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlD_fJueZOYg"
      },
      "source": [
        "Схема DF доступна через атрибут `schema`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KL1CKgKXZOYg"
      },
      "outputs": [],
      "source": [
        "schema = df.schema\n",
        "schema.simpleString()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsoT33VOZOYg"
      },
      "outputs": [],
      "source": [
        "df.schema[\"type\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKeU3IoiZOYh"
      },
      "outputs": [],
      "source": [
        "foo = df.schema[\"type\"]\n",
        "foo.dataType"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gX--xCHZOYh"
      },
      "source": [
        "Если указать схему при чтении источника, то spark не будет пытаться определить ее автоматически, что, в случае работы с такими типами файлов, как `csv` и `json`, сократит время создания `df`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDhb9jc2ZOYi"
      },
      "outputs": [],
      "source": [
        "df = spark.read.format(\"csv\") \\\n",
        "        .schema(schema) \\\n",
        "        .options(header=True, inferSchema=False) \\\n",
        "        .load(\"/tmp/datasets/airport-codes.csv\")\n",
        "\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpaTVHt-ZOYi"
      },
      "source": [
        "Схема может быть создана вручную:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lel-AMJSZOYj"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType\n",
        "\n",
        "my_schema = \\\n",
        "    StructType(\n",
        "        [\n",
        "            StructField(\"foo\", StringType()),\n",
        "            StructField(\"bar\", StringType()),\n",
        "            StructField(\n",
        "                        \"boo\", \n",
        "                        StructType(\n",
        "                            [\n",
        "                                StructField(\"x\", IntegerType()),\n",
        "                                StructField(\"y\", BooleanType())\n",
        "                            ]\n",
        "                            )\n",
        "                       )\n",
        "        ]\n",
        "    )\n",
        "\n",
        "my_schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh7o1e_FZOYj"
      },
      "source": [
        "### Выводы:\n",
        "+ Spark использует схемы для описания типов колонок, схемы всего DF, чтения источников и для работы с JSON\n",
        "+ Схема представляет собой инстанс класса `StructType`\n",
        "+ Колонки в Spark могут иметь любой тип. При этом вложенность словарей, массивов и структур не ограничена\n",
        "\n",
        "## Оптимизатор запросов Catalyst\n",
        "Catalyst выполняет оптимизацию запросов с целью ускорения их выполнения и применяет следующие методы:\n",
        " + Column projection\n",
        " + Partition pruning\n",
        " + Predicate pushdown\n",
        " + Constant folding\n",
        " \n",
        " Подготовим датасет для демонстрации работы Catalyst:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fejGNQa7ZOYk"
      },
      "outputs": [],
      "source": [
        "df \\\n",
        "    .write \\\n",
        "    .format(\"parquet\") \\\n",
        "    .partitionBy(\"iso_country\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(\"/tmp/airports.parquet\") \\\n",
        "\n",
        "airports = spark.read.parquet(\"/tmp/airports.parquet\")\n",
        "airports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1wuxOakZOYl"
      },
      "outputs": [],
      "source": [
        "!hdfs dfs -ls /tmp/airports.parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lITO-shBZOYl"
      },
      "source": [
        "### Column projection\n",
        "Данный механизм позволяет избегать вычитывания ненужных колонок при работе с источниками"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HoDjQaBZOYl"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "selected = airports.select(col(\"ident\"))\n",
        "selected.cache()\n",
        "selected.count()\n",
        "selected.unpersist()\n",
        "selected.explain(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccIdDfCqZOYm"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "selected = airports\n",
        "selected.cache()\n",
        "selected.count()\n",
        "selected.unpersist()\n",
        "selected.explain(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0DS-TpHZOYm"
      },
      "source": [
        "### Partition pruning\n",
        "Данный механизм позволяет избежать чтения ненужных партиций"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1gEOGDdZOYn"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "filtered = airports.filter(col(\"iso_country\") == \"RU\")\n",
        "filtered.count()\n",
        "filtered.explain(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnOuBdYwZOYn"
      },
      "source": [
        "### Predicate pushdown\n",
        "Данный механизм позволяет \"протолкнуть\" условия фильтрации данных на уровень datasource"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Z5zgXAZZOYo"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "filtered = airports.filter(col(\"iso_region\") == \"RU\")\n",
        "filtered.count()\n",
        "filtered.explain(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtYW0ji_ZOYo"
      },
      "source": [
        "### Simplify casts\n",
        "Данный механизм убирает ненужные `cast`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vH7YgZ14ZOYp"
      },
      "outputs": [],
      "source": [
        "result = spark.range(10).select(col(\"id\").cast(\"long\"))\n",
        "result.explain(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3Aog5SOZOYp"
      },
      "outputs": [],
      "source": [
        "result = spark.range(10).select(col(\"id\").cast(\"int\").cast(\"long\"))\n",
        "result.explain(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q6oD8pdZOYp"
      },
      "source": [
        "### Constant folding\n",
        "Данный механизм сокращает количество констант, используемых в физическом плане"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hmb04lSNZOYq"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import lit\n",
        "\n",
        "result = spark.range(10).select((lit(3) >  lit(0)).alias(\"foo\"))\n",
        "result.explain(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVo3WGguZOYq"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import lit, col\n",
        "\n",
        "result = spark.range(10).select((col(\"id\") >  lit(0)).alias(\"foo\"))\n",
        "result.explain(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZKd29q2ZOYr"
      },
      "source": [
        "### Combine filters\n",
        "Данный механизм объединяет фильтры"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVWp7av-ZOYr"
      },
      "outputs": [],
      "source": [
        "result = spark.range(10).filter(col(\"id\") > 0).filter(col(\"id\") != 5).filter(col(\"id\") < 10)\n",
        "result.explain(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxNTvZBVZOYs"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "spark_advanced.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}